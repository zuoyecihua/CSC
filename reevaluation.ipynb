{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abd4f105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\论文代码和数据备份\\CSC\\utils\\args_util.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  args = DottedDict(yaml.load(open(yaml_path, \"r\"), Loader=None))\n",
      "D:\\论文代码和数据备份\\CSC\\utils\\args_util.py:7: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  args = DottedDict(yaml.load(open(yaml_path, \"r\"), Loader=None))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\论文代码和数据备份\\CSC\n",
      "279379 samples found in ./resources/merged/clean_extend_training_input_shuf_v1.txt\n",
      "D:\\论文代码和数据备份\\CSC\n",
      "1100 samples found in ./resources/sighan15/TestInput.txt\n",
      "load saved tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast,BertModel\n",
    "from data.call_func import _get_tokenizer\n",
    "from dotted_dict import DottedDict\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from data.call_func import sighan_error_correct_prompt,sighan_error_detec_csc\n",
    "\n",
    "from data.data_entry import get_loader, get_dataset\n",
    "from model.model_entry import select_model\n",
    "\n",
    "from utils.args_util import load_args\n",
    "from trainers.train_corrector import TrainerCorrector\n",
    "from optimizer import get_optimizer\n",
    "from data.utils import build_candidates_dict\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from metrics import compute_csc_metrics_by_file\n",
    "torch.set_printoptions(precision=2)\n",
    "from data.utils import word2pinyin_similar\n",
    "from data.call_func import _get_tokenizer\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "#加载训练参数\n",
    "config_yaml = \"test_csc.yaml\"\n",
    "args = load_args(\"./configs/%s\"%config_yaml)\n",
    "train_dataset = get_dataset(args.dataset.train)\n",
    "test_dataset = get_dataset(args.dataset.test)\n",
    "_tokenizer = _get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f997346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\论文代码和数据备份\\CSC\\model\\copy_transformers\\configuration_utils.py:336: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(21954, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#加载detec模型\n",
    "device = \"cpu\"\n",
    "config_yaml = \"test_csc.yaml\"\n",
    "args = load_args(\"./configs/%s\"%config_yaml)\n",
    "detec_model = select_model(args.model).to(device)\n",
    "detec_model.encoder.resize_token_embeddings(len(_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16e1e7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21128"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import  BertTokenizer\n",
    "len(BertTokenizer.from_pretrained(\"bert-base-chinese\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "253126ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_tokenize(self, text):\n",
    "    # print(\"use my tokenizer !!!\")\n",
    "    return list(text.lower())\n",
    "import types\n",
    "from transformers import BertTokenizer\n",
    "BertTokenizer.my_tokenize = my_tokenize\n",
    "\n",
    "# detec_model.load_state_dict(torch.load(\"./saved/pinyin_corrector/20211104150912/model_epoch18_f10.772646531414744.pkl\",map_location=\"cpu\")['model_state_dict'])\n",
    "detec_model.load_state_dict(torch.load(\"./saved/pinyin_corrector/20211104150721/model_epoch4_f10.767172162709159.pkl\",map_location=\"cpu\")['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ce12f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "瓠互沪楛沍护扈\t壶胡鹄蝴胡琥湖葫囫弧虎许斛瑚惚唬乎戏呼糊忽狐\t富腹馥覆赙附副傅复付复赴妇咐父阜赋负驸讣\t氟缚匐肤𫓧俯府腑夫腐𫖯蝠绋扶甫幅芾孚弗辐绂福袱浮伏符菔釜敷拂涪抚怫斧孵莆趺辅佛服父麸脯芙伕俘\n"
     ]
    }
   ],
   "source": [
    "word2candidates = build_candidates_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af1ff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_shape_candidates_dict():\n",
    "    #取得一个汉字的近形字和近音字\n",
    "    word2candidates = defaultdict(set)\n",
    "    with open(r\"resources/gcn_graph.ty_xj/SimilarShape_simplied.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        for l in  lines:\n",
    "            w,candiates = [x.strip() for x in l.split(\",\",1)]\n",
    "            word2candidates[w].update(candiates)\n",
    "    return word2candidates\n",
    "\n",
    "word2shape_candidates = build_shape_candidates_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c54c1993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_candidates(text_file,label_file):\n",
    "    w2c = defaultdict(set)\n",
    "    f1,f2 = open(text_file,\"r\",encoding=\"utf-8\"),open(label_file,\"r\",encoding=\"utf-8\")\n",
    "    for text,label in zip(list(f1),list(f2)):\n",
    "        text = text.split(\"\\t\")[1].strip()\n",
    "        label_splits = label.strip().split(\",\")\n",
    "        if label_splits[-1] == \"\":\n",
    "            del label_splits[-1]\n",
    "        if len(label_splits) < 3:\n",
    "            continue\n",
    "        for i in range(1,len(label_splits),2):\n",
    "            pos,candidate = label_splits[i],label_splits[i+1]\n",
    "            idx = int(pos) - 1\n",
    "            w2c[text[idx]].add(candidate.strip())\n",
    "    return w2c\n",
    "#从训练集中提取候选集\n",
    "add_w2c = add_candidates(\"./resources/merged/clean_extend_training_input_shuf_v1.txt\",\"./resources/merged/clean_extend_training_truth_shuf_v1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0193451a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'堤', '在', '底', '得', '递', '低', '迪', '滴', '多', '的', '弟', '帝', '第', '抵'}\n"
     ]
    }
   ],
   "source": [
    "print(add_w2c[\"地\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1590318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(data):\n",
    "        inputs = data\n",
    "        for k in inputs.keys():\n",
    "            if isinstance(inputs[k], torch.Tensor):\n",
    "                inputs[k] = inputs[k].to(device)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24beb286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-4dd7f5d99b19>:41: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for l in tqdm_notebook(list(f)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de7702d35884eaea77c469566e21016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/279379 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def write_csc_predictions( input_ids, predict_labels, sentence_ids, save_path, is_13 = False):\n",
    "    tokenizer = _get_tokenizer()\n",
    "    comma_token_id = tokenizer.convert_tokens_to_ids([\",\"])[0]\n",
    "    if not os.path.exists(os.path.dirname(save_path)):\n",
    "        os.makedirs(os.path.dirname(save_path))\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # print(len(input_ids),len(predict_labels),len(sentence_ids))\n",
    "        punctuations = ['\"', '”', \">\", \"》\", \"]\", \"】\"]\n",
    "        for tl,pl,sid in zip(input_ids,predict_labels,sentence_ids):\n",
    "            corrected = [sid]\n",
    "            for i in range(len(tl)):\n",
    "                if tl[i] == tokenizer.sep_token_id:\n",
    "                    break\n",
    "                if tl[i] == pl[i]:\n",
    "                    continue\n",
    "                pred_token = tokenizer.convert_ids_to_tokens([pl[i]])[0].strip()\n",
    "                if len(pred_token) != 1 or pred_token in punctuations:\n",
    "                    continue\n",
    "                true_token = tokenizer.convert_ids_to_tokens( tl[i] )[0].strip()\n",
    "                if pred_token not in word2candidates[true_token] and true_token not in word2candidates[pred_token] and true_token not in add_w2c[pred_token] and pred_token not in add_w2c[true_token] and word2pinyin_similar(true_token) != word2pinyin_similar(pred_token):\n",
    "#                     print(pred_token,true_token)\n",
    "                    continue\n",
    "                if pred_token in \"得的地\" and is_13: #remove \"的得地\" in sighan13\n",
    "                    continue\n",
    "                corrected.extend([i, pred_token])\n",
    "            if len(corrected) == 0:\n",
    "                corrected.append(0)\n",
    "            corrected_str = \", \".join([str(c) for c in corrected]) + \"\\n\"\n",
    "            f.write(corrected_str)\n",
    "            \n",
    "def get_candidates_all(w):\n",
    "    results = []\n",
    "    results += list(word2candidates[w]) + list(add_w2c[w])\n",
    "    return list (set(results))\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook\n",
    "#统计词频\n",
    "wcnt = Counter()\n",
    "with open(\"./resources/merged/clean_extend_training_input_shuf_v1.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    for l in tqdm_notebook(list(f)):\n",
    "        for i in range(len(l) - 1):\n",
    "            wcnt.update([l[i:i+2]])\n",
    "            \n",
    "\n",
    "#使用候选集框定正确的汉字\n",
    "def predict(text,use_pinyin=True):\n",
    "        id = \"\"\n",
    "        label = \"\"\n",
    "        raw_item =(id,text,label)\n",
    "        inputs = sighan_error_detec_csc([raw_item])\n",
    "        if not use_pinyin:\n",
    "            inputs['pinyin_ids'] = None\n",
    "        inputs = step(inputs)\n",
    "        detec_logits = detec_model(inputs)\n",
    "        return detec_logits.softmax(dim=2)\n",
    "def get_candidates(w):\n",
    "#     print(\"retrieve candidates:\", w)\n",
    "    tokens = list(set(list(word2candidates[w] )+list(add_w2c[w])))\n",
    "    ids = torch.LongTensor(tokenizer.convert_tokens_to_ids(tokens)).to(device)\n",
    "    return tokens,ids\n",
    "def get_err_logit(logits, predict_text, original_text):\n",
    "    logit = 1\n",
    "    similarity = 0\n",
    "    cnt = 0\n",
    "    for i in range(len(original_text)):\n",
    "        oc = original_text[i]\n",
    "        pc = predict_text[i]\n",
    "        if oc == pc:\n",
    "            continue\n",
    "        logit *= logits[i+1][tokenizer.convert_tokens_to_ids([pc])[0]].item()\n",
    "        cnt += 1\n",
    "        similarity += get_similar_score(oc,pc)\n",
    "    return logit, similarity\n",
    "\n",
    "\n",
    "def get_similar_score(a,b):\n",
    "    def clean_pinyin(x):\n",
    "        return x.replace(\"an\",\"a\").replace(\"en\",\"e\").replace(\"on\",\"o\").replace(\"in\",\"i\")\n",
    "    a = a.replace(\"地\",\"的\")\n",
    "    b =  b.replace(\"地\",\"的\")\n",
    "    pinyins_a = [clean_pinyin (word2pinyin_similar(x) ) for x in a]\n",
    "    pinyins_b = [clean_pinyin(word2pinyin_similar(x)) for x in b]\n",
    "    pinyin_score = 0\n",
    "    for p in pinyins_a:\n",
    "        if p in pinyins_b:\n",
    "            pinyin_score += 1\n",
    "    score = pinyin_score \n",
    "    \n",
    "    shape_score = 0\n",
    "    for x,y in zip(a,b):\n",
    "        if x in word2shape_candidates[y] or y in word2shape_candidates[x]:\n",
    "            shape_score += 1\n",
    "    score += shape_score\n",
    "    return score\n",
    "\n",
    "def is_good_text(text):\n",
    "    logits = predict(text)\n",
    "    ids = logits.argmax(dim=2)\n",
    "    token_logits = logits.gather(2,ids[:,:,None]).squeeze()\n",
    "    if token_logits.min() < 0.5:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_top_tokens(t, l,ids):\n",
    "    return np.array(t)[l[ids].argsort(descending=True).cpu().numpy()][:5].tolist()\n",
    "\n",
    "def get_top_tokens_by_pinyin(t,l,ids, pinyin_code):\n",
    "    lst =  np.array(t)[l[ids].argsort(descending=True).cpu().numpy()].tolist()\n",
    "    lst1 = [c for c in lst if word2pinyin_similar(c) == pinyin_code]\n",
    "    lst2 = [c for c in lst if word2pinyin_similar(c) != pinyin_code]\n",
    "    return lst1[:3] + lst2[:3]\n",
    "\n",
    "def possible_err_positions(logits):\n",
    "    ids = logits.argmax(dim=2)\n",
    "    token_logits = logits.gather(2,ids[:,:,None]).squeeze()\n",
    "#     print(token_logits)\n",
    "    tensor_ids = torch.where( token_logits < 0.8 )[0]\n",
    "    if len(tensor_ids) == 1:\n",
    "        device = tensor_ids.device\n",
    "        tensor_ids = tensor_ids.item()\n",
    "        if token_logits[tensor_ids-1]   <  0.95 and (token_logits[tensor_ids-1] * token_logits[tensor_ids]) < 0.64:\n",
    "            tensor_ids = torch.LongTensor([tensor_ids-1,(tensor_ids)]).to(device)\n",
    "        elif  token_logits[tensor_ids+1] < 0.95 and (token_logits[tensor_ids] * token_logits[tensor_ids+1]) < 0.64:\n",
    "            tensor_ids = torch.LongTensor([tensor_ids,(tensor_ids+1)]).to(device)\n",
    "        else:\n",
    "            tensor_ids =  torch.LongTensor([tensor_ids]).to(device)\n",
    "    return  tensor_ids \n",
    "\n",
    "def is_bi_err(tensor_ids):\n",
    "    ids =tensor_ids.tolist()\n",
    "    return len(ids)==2 and ids[0] + 1== ids[1]\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "    \n",
    "\n",
    "def solve_bi_err(index,logits,original_text, target=0.9):\n",
    "    detec_predicts = logits.argmax(dim=2)\n",
    "    predict_text = \"\".join(tokenizer.convert_ids_to_tokens(detec_predicts[0][1:-1]))\n",
    "    print(\"predict_text:\",predict_text)\n",
    "    tokens = list(predict_text)\n",
    "    l1,l2 = logits[0][index]\n",
    "    p1,p2 = index[0]-1, index[1]-1\n",
    "    \n",
    "\n",
    "    t1,ids1 = get_candidates(original_text[index[0]-1])\n",
    "    t2,ids2 = get_candidates(original_text[index[1]-1])\n",
    "    \n",
    "#     t1_pinyin, t1_other = [c for c in t1 if word2pinyin_similar(c) == word2pinyin_similar(original_text[index[0]-1])],  \\ \n",
    "#     [c for c in t1 if word2pinyin_similar(c) != word2pinyin_similar(original_text[index[0]-1])]\n",
    "    \n",
    "#     t2_pinyin, t2_other = [c for c in t2 if word2pinyin_similar(c) == word2pinyin_similar(original_text[index[1]-1])],  \\ \n",
    "#     [c for c in t2 if word2pinyin_similar(c) != word2pinyin_similar(original_text[index[1]-1])]\n",
    "    c1 =[original_text[p1],predict_text[p1]]  + get_top_tokens_by_pinyin(t1,l1,ids1,word2pinyin_similar(original_text[p1]))\n",
    "    c2 =[original_text[p2],predict_text[p2]] + get_top_tokens_by_pinyin(t2,l2,ids2,word2pinyin_similar(original_text[p2]))\n",
    "#     c1 =[predict_text[p1]] + get_top_tokens(t1,l1,ids1)\n",
    "#     c2 =[predict_text[p2]] + get_top_tokens(t2,l2,ids2)\n",
    "    print(c1,c2)\n",
    "    correct_predict_ids = None\n",
    "    correct_text = None\n",
    "    max_logit = 0\n",
    "    similar_score = 0\n",
    "    total_score = 0\n",
    "    \n",
    "    absolute_good_candidates = []\n",
    "    for s1,s2 in list(product(c1,c2)):\n",
    "        #原则：优先选概率高和相似度高的\n",
    "#         wf = min([wcnt[x] for x in wcnt])\n",
    "#         print( s1+s2 ,predict_text[p1]+predict_text[p2],wcnt[s1+s2])\n",
    "        if wcnt[s1+s2] < 3 and s1+s2 != predict_text[p1]+predict_text[p2]:\n",
    "            continue\n",
    "        \n",
    "        tokens[p1] = s1\n",
    "        tokens[p2] = s2\n",
    "        text = \"\".join(tokens)\n",
    "        logits = predict(text, use_pinyin=True) \n",
    "        ids = logits.argmax(dim=2)\n",
    "        token_logits = logits.gather(2,ids[:,:,None]).squeeze()\n",
    "        new_text = \"\".join(tokenizer.convert_ids_to_tokens(ids.cpu().tolist()[0])[1:-1])\n",
    "        no_deep_modify = text==new_text#text[:p1]==new_text[:p1] and text[p2+1]==new_text[p2+1]\n",
    "#         print(s1+s2,no_deep_modify,text,new_text)\n",
    "\n",
    "        if no_deep_modify :\n",
    "            reselect_score = 0\n",
    "            new_similar_score = get_similar_score(original_text[p1:p2+1],s1+s2)\n",
    "\n",
    "            reselect_score += new_similar_score\n",
    "            if wcnt[s1+s2] > 1000:\n",
    "                reselect_score +=1\n",
    "            new_logit = token_logits[p1+1] * token_logits[p2+1]\n",
    "            if new_logit> target:\n",
    "                absolute_good_candidates.append((new_text, new_logit.item(),s1+s2, reselect_score,wcnt[s1+s2],  ids.cpu().numpy().tolist()))\n",
    "    print(correct_text,max_logit)\n",
    "\n",
    "    if len(absolute_good_candidates) == 0:\n",
    "        return detec_predicts\n",
    "    else:\n",
    "        absolute_good_candidates = sorted(absolute_good_candidates,key=lambda x: (-x[3], -x[1]))\n",
    "    print(absolute_good_candidates[0][:5])\n",
    "    return absolute_good_candidates[0][-1]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def check_text(text, poses):\n",
    "    logits = predict(predict_text)\n",
    "    ids = logits.argmax(dim=2)\n",
    "    token_logits = logits.gather(2,ids[:,:,None]).squeeze()\n",
    "    min_logit = min([token_logits[i+1] for i in poses])\n",
    "    return min_logit\n",
    "    \n",
    "def solve_single_err(index,logits,original_text):\n",
    "    \n",
    "    detec_predicts = logits.argmax(dim=2)\n",
    "    predict_text = \"\".join(tokenizer.convert_ids_to_tokens(detec_predicts[0][1:-1]))\n",
    "    tokens = list(predict_text)\n",
    "    \n",
    "    l1 = logits[0][index][0]\n",
    "    p1 = index[0]-1\n",
    "    #确认修改后的汉字没问题的话，直接返回\n",
    "#     min_logit = check_text(predict_text,[p1])\n",
    "# #     print(\"min_logit:\",min_logit)\n",
    "#     if min_logit > 0.5:\n",
    "#         return None, None\n",
    "#     else:\n",
    "#         print(predict_text[p1],\":\",min_logit.item())\n",
    "    if original_text[p1] in get_candidates_all(predict_text[p1]) or predict_text[p1] in get_candidates_all(original_text[p1]) or predict_text[p1] == original_text[p1]:\n",
    "        return None, None\n",
    "    \n",
    "    t1, ids1 = get_candidates(original_text[p1])\n",
    "    c1 =[original_text[p1]] + [ predict_text[p1]] + get_top_tokens(t1,l1,ids1)\n",
    "    \n",
    "    print(\"predict_text:\",predict_text, \"err:\",predict_text[p1] )\n",
    "\n",
    "    logit = 0\n",
    "    similar_score = 0\n",
    "    logit = 0\n",
    "    correct_text = None\n",
    "    correct_ids = None\n",
    "    \n",
    "    all_candidate_sentence = []\n",
    "    for s1 in c1:\n",
    "        tokens[p1] = s1\n",
    "        text = \"\".join(tokens)\n",
    "        logits = predict(text)\n",
    "        ids = logits.argmax(dim=2)\n",
    "        token_logits = logits.gather(2,ids[:,:,None]).squeeze()\n",
    "        new_text = \"\".join(tokenizer.convert_ids_to_tokens(ids.cpu().tolist()[0])[1:-1])\n",
    "        span = original_text[p1-1:p1+2]\n",
    "        new_span = new_text[p1-1:p1+2]\n",
    "        cnt = 0\n",
    "\n",
    "            \n",
    "#         if span[0] != new_span[0] or span[-1] !=new_span[-1]:\n",
    "            \n",
    "#             no_deep_modify = text[p1]==new_text[p1] \n",
    "        new_similar_score = get_similar_score(span,new_span)\n",
    "        print(span,new_span, get_similar_score(span,new_span))\n",
    "#         for s,b in zip(span,new_span):\n",
    "#             if a!=b:\n",
    "#                 cnt += 1\n",
    "#         if cnt <=1:\n",
    "#             print(cnt)\n",
    "#             continue\n",
    "        new_logit = token_logits[p1+1]\n",
    "#         print(s1, \"%.2f\"%new_logit.item(),new_text,new_similar_score)\n",
    "        if new_logit > logit and new_similar_score >= similar_score:\n",
    "            correct_text, logit, similar_score,correct_ids = new_text, new_logit, new_similar_score,ids\n",
    "        elif new_logit - logit > - 0.01 and new_similar_score > similar_score:\n",
    "            correct_text, logit, similar_score,correct_ids = new_text, new_logit, new_similar_score,ids\n",
    "            \n",
    "    return correct_text,correct_ids\n",
    "\n",
    "def check_continue_err(text1,text2):\n",
    "    errs = []\n",
    "    for i in range(len(text1)-1):\n",
    "        if text1[i] != text2[i] and text1[i+1] != text2[i+1]:\n",
    "            return [i+1, i+2]\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a55b41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59c3c6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\论文代码和数据备份\\CSC\n",
      "1100 samples found in ./resources/sighan15/TestInput.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-76d0a40e71fd>:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(len(test_dataset))):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5245e58275744206886fb8b60b300a3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理连续错误： 我以前想要高诉你，可是我忘了。我真户秃。\n",
      "predict_text: 我以前想要告诉你，可是我忘了。我真感歉。\n",
      "['户', '感', '胡', '糊', '狐', '尸', '富', '敷'] ['秃', '歉', '涂', '突', '吐', '秀', '妒', '稚']\n",
      "None 0\n",
      "('我以前想要告诉你，可是我忘了。我真糊涂。', 0.999961256980896, '糊涂', 2, 23)\n",
      "\n",
      "开始处理连续错误： 上年春天我到台湾东部到台东去看到我的好朋友，因为我们面件很久。\n",
      "predict_text: 上年春天我到台湾东部到台东去看到我的好朋友，因为我们面见很久。\n",
      "['面', '面', '面', '沔', '偭', '们', '夤', '栘'] ['件', '见', '见', '肩', '间', '签', '价', '鲜']\n",
      "None 0\n",
      "('上年春天我到台湾东部到台东去看到我的好朋友，因为我们面见很久。', 0.9993343949317932, '面见', 3, 11)\n",
      "\n",
      "开始处理连续错误： 可以问路的女人是他的是新中文老师。\n",
      "predict_text: 可以问路的女人是他的是新中文老师。\n",
      "['是', '是', '实', '世', '师', '摄', '次', '定'] ['新', '新', '心', '信', '姓', '乡', '津', '敬']\n",
      "None 0\n",
      "\n",
      "开始处理连续错误： 你可以告诉我那家书店有中文数马？\n",
      "predict_text: 你可以告诉我那家书店有中文课吗？\n",
      "['数', '课', '书', '输', '素', '硕', '锁', '说'] ['马', '吗', '吗', '嘛', '码', '么', '验', '𫘪']\n",
      "None 0\n",
      "('你可以告诉我那家书店有中文书吗？', 0.9985228776931763, '书吗', 3, 4)\n",
      "\n",
      "开始处理连续错误： 可是很多外国人很喜欢赛太杨在海边，在印尼有很多外国人来路性。\n",
      "predict_text: 可是很多外国人很喜欢晒太阳在海边，在印尼有很多外国人来旅行。\n",
      "['路', '旅', '旅', '露', '绿', '女', '跳', '度'] ['性', '行', '行', '信', '兴', '金', '精', '牛']\n",
      "None 0\n",
      "('可是很多外国人很喜欢晒太阳在海边，在印尼有很多外国人来旅行。', 0.9999954700469971, '旅行', 2, 471)\n",
      "\n",
      "开始处理连续错误： 台湾是一个很美的国家，共公架构也做的很好。\n",
      "predict_text: 台湾是一个很美的国家，公公架构也做得很好。\n",
      "['共', '公', '公', '工', '供', '益', '空', '基'] ['公', '公', '共', '功', '工', '分', '控', '釭']\n",
      "None 0\n",
      "('台湾是一个很美的国家，供工架构也做得很好。', 0.946269690990448, '供工', 3, 3)\n",
      "\n",
      "开始处理连续错误： 他再也不会撤扬。\n",
      "predict_text: 它再也不会撤扬。\n",
      "['撤', '撤', '侧', '坼', '策', '撒', '散', '敞'] ['扬', '扬', '样', '杨', '养', '挥', '场', '招']\n",
      "None 0\n",
      "\n",
      "开始处理连续错误： 我去握放，我拿东西回去给我的朋友。\n",
      "predict_text: 我去派放，我拿东西回去给我的朋友。\n",
      "['握', '派', '我', '卧', '窝', '挥', '套', '倒'] ['放', '放', '房', '饭', '坊', '坟', '份', '杭']\n",
      "None 0\n",
      "('我去套房，我拿东西回去给我的朋友。', 0.9990903735160828, '套房', 3, 86)\n",
      "\n",
      "开始处理连续错误： 这销析对你来说应该是很刺刺的，可是你也应该了解我的点子。\n",
      "predict_text: 这消息对你来说应该是很刺刺的，可是你也应该了解我的点子。\n",
      "['销', '消', '消', '效', '小', '角', '脚', '觉'] ['析', '息', '息', '戏', '悉', '辑', '果', '绩']\n",
      "None 0\n",
      "('这消息对你来说应该是很刺刺的，可是你也应该了解我的点子。', 0.999976396560669, '消息', 4, 2876)\n",
      "\n",
      "开始处理连续错误： 你的儿子敬启。\n",
      "predict_text: 你的儿子敬启。\n",
      "['敬', '敬', '静', '金', '锦', '钦', '馨', '秦'] ['启', '启', '琦', '祺', '琪', '敬', '霁', '敏']\n",
      "None 0\n",
      "\n",
      "开始处理连续错误： 因为现在各各国家的经济情况越来越互相关联，不管你在哪里，也不管你是做什么工作的，经济不好的时候都会失业的可能性。\n",
      "predict_text: 因为现在个各国家的经济情况越来越互相关联，不管你在哪里，也不管你是做什么工作的，经济不好的时候都会失业的可能性。\n",
      "['各', '个', '个', '各', '隔', '同', '合', '棵'] ['各', '各', '各', '个', '隔', '合', '同', '路']\n",
      "None 0\n",
      "('因为现在各路国家的经济情况越来越互相关联，不管你在哪里，也不管你是做什么工作的，经济不好的时候都会失业的可能性。', 0.9987403154373169, '各路', 3, 20)\n",
      "\n",
      "开始处理连续错误： 因为你是可塑高的人，没什么困染可以让你退步，对不对？\n",
      "predict_text: 因为你是可塑高的人，没什么困难可以让你退步，对不对？\n",
      "['塑', '塑', '树', '素', '输', '望', '填', '钢'] ['高', '高', '搞', '告', '稿', '考', '够', '乔']\n",
      "None 0\n",
      "\n",
      "开始处理连续错误： 因为他花太多时间跟朋友们玩电脑游戏，怕他的成绩会差。我爸妈每次劳到他要做功课。\n",
      "predict_text: 因为他花太多时间跟朋友们玩电脑游戏，怕他的成绩会差。我爸妈每次唠到他要做功课。\n",
      "['劳', '唠', '唠', '老', '牢', '聊', '弄', '恼'] ['到', '到', '叨', '道', '倒', '打', '制', '头']\n",
      "None 0\n",
      "('因为他花太多时间跟朋友们玩电脑游戏，怕他的成绩会差。我爸妈每次唠叨他要做功课。', 0.9998931884765625, '唠叨', 3, 6)\n",
      "\n",
      "开始处理连续错误： 我妳那情的时候，也在您的工厂打工。\n",
      "predict_text: 我妳那年的时候，也在您的工厂打工。\n",
      "['那', '那', '拿', '哪', '纳', '年', '难', '男'] ['情', '年', '轻', '请', '清', '静', '持', '进']\n",
      "None 0\n",
      "('我妳年轻的时候，也在您的工厂打工。', 0.9999613761901855, '年轻', 1, 737)\n",
      "\n",
      "开始处理连续错误： 父母要了解他们不应该干涉教育过程、批评老师的教法，是因为设施校长的工作。\n",
      "predict_text: 父母要了解他们不应该干涉教育过程、批评老师的教法，是因为涉施校长的工作。\n",
      "['设', '涉', '涉', '舍', '摄', '没', '这', '误'] ['施', '施', '事', '是', '实', '于', '妨', '仿']\n",
      "None 0\n",
      "('父母要了解他们不应该干涉教育过程、批评老师的教法，是因为这是校长的工作。', 0.9999179840087891, '这是', 2, 4338)\n",
      "\n",
      "detect_precision=0.812332, detect_recall=0.847552, detect_Fscore=0.829569\n",
      "correct_precision=0.980198, correct_recall=0.830769, correct_Fscore=0.899319\n",
      "dc_joint_precision=1.000000, dc_joint_recall=1.000000, dc_joint_Fscore=1.000000\n",
      "detect_sent_precision=0.807339, detect_sent_recall=0.800000, detect_Fscore=0.803653\n",
      "correct_sent_precision=0.796330, correct_sent_recall=0.789091, correct_Fscore=0.792694\n"
     ]
    }
   ],
   "source": [
    "#sighan15\n",
    "tokenizer = _get_tokenizer()\n",
    "predict_labels = []\n",
    "input_ids = []\n",
    "sentence_ids = []\n",
    "sig = \"15\"\n",
    "test_data_arg = args.dataset.test\n",
    "test_data_arg.text_path = \"./resources/sighan{}/TestInput.txt\".format(sig)\n",
    "test_data_arg.label_path = \"./resources/sighan{}/TestTruth.txt\".format(sig)\n",
    "\n",
    "test_dataset = get_dataset(test_data_arg)\n",
    "from tqdm  import tqdm_notebook\n",
    "with torch.no_grad():\n",
    "    for i in tqdm_notebook(range(len(test_dataset))):\n",
    "        raw_item = test_dataset.__getitem__(i)\n",
    "        original_text = raw_item[1]\n",
    "        inputs = sighan_error_detec_csc([raw_item])\n",
    "        inputs = step(inputs)\n",
    "        detec_logits = detec_model(inputs)\n",
    "        detec_predicts = detec_logits.argmax(dim=2).cpu().numpy().tolist()\n",
    "        detec_tokens = tokenizer.convert_ids_to_tokens(detec_predicts[0])[1:-1]\n",
    "        \n",
    "        \n",
    "        #对连续错误进行处理\n",
    "        predict_text = \"\".join(detec_tokens)\n",
    "        logits = detec_logits.softmax(dim=2)\n",
    "        tensor_ids = possible_err_positions(logits)\n",
    "\n",
    "\n",
    "        new_error_outer = False\n",
    "        for i in range(len(original_text) -1):\n",
    "            if original_text[i] != predict_text[i] and original_text[i+1] != predict_text[i+1] and (predict_text[i] not in get_candidates(original_text[i])[0]  or predict_text[i+1] not in get_candidates(original_text[i+1])[0]):\n",
    "                import re\n",
    "                if len(re.findall(r'([\\u2E80-\\u9FFF]+)',original_text[i] )) == 0:\n",
    "                    continue\n",
    "                new_error_outer = True\n",
    "                tensor_ids = torch.LongTensor([i+1,i+2]).to(tensor_ids.device)\n",
    "                break\n",
    "                \n",
    "        target = 0.9\n",
    "\n",
    "        if len(tensor_ids) == 2 and tensor_ids[0]+ 1 == tensor_ids[1] or new_error_outer:\n",
    "            print(\"开始处理连续错误：\", raw_item[1])\n",
    "            modified_predict_ids = solve_bi_err(tensor_ids,logits,raw_item[1],target)\n",
    "            if modified_predict_ids is not None:\n",
    "                detec_predicts = modified_predict_ids\n",
    "            print()\n",
    "        \n",
    "            \n",
    "        tokenizer = _get_tokenizer()\n",
    "        \n",
    "        input_ids.extend(inputs['input_ids'].detach().cpu().numpy().tolist())\n",
    "  \n",
    "\n",
    "        predict_labels.extend(detec_predicts)\n",
    "        \n",
    "        sentence_ids.extend(inputs['sentence_ids'])\n",
    "        \n",
    "        predict_text = \"\".join(tokenizer.convert_ids_to_tokens(detec_predicts[0][1:-1]))\n",
    "\n",
    "\n",
    "save_path = \"../test_file\"\n",
    "write_csc_predictions(input_ids, predict_labels, sentence_ids, save_path)\n",
    "metrics = compute_csc_metrics_by_file(save_path,\"./resources/sighan{}/TestTruth.txt\".format(sig), show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e6679cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\论文代码和数据备份\\CSC\n",
      "1062 samples found in ./resources/sighan14/TestInput.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-ef40241649e9>:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(len(test_dataset))):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0272159c029642f2a7928c97e1f28e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理连续错误： 在次，我想最重要的还是能于培养出一群杰出的资源，能于为国家争荣、愿意贡献一份力量的人才。\n",
      "predict_text: 在此，我想最重要的还是能于培养出一群杰出的资源，能于为国家争荣、愿意贡献一份力量的人才。\n",
      "['在', '在', '再', '载', '宰', '台', '才', '子'] ['次', '此', '此', '刺', '魑', '思', '款', '子']\n",
      "None 0\n",
      "('在此，我想最重要的还是能于培养出一群杰出的资源，能于为国家争荣、愿意贡献一份力量的人才。', 0.9973002672195435, '在此', 3, 1623)\n",
      "\n",
      "开始处理连续错误： 因为妈妈是给我出生的人，养我从小到大，她找钱得很辛苦，为了养小孩，照顾孩子，给小孩读书毕业，现在我作工了我更体会她的劳苦劳累。\n",
      "predict_text: 因为妈妈是给我出生的人，养我从小到大，她赚钱得很辛苦，为了养小孩，照顾孩子，给小孩读书毕业，现在我做工了我更体会她的劳苦劳累。\n",
      "['作', '做', '做', '坐', '佐', '任', '你', '非'] ['工', '工', '功', '公', '供', '空', '红', '害']\n",
      "None 0\n",
      "('因为妈妈是给我出生的人，养我从小到大，她赚钱得很辛苦，为了养小孩，照顾孩子，给小孩读书毕业，现在我做功了我更体会她的劳苦劳累。', 0.9966109395027161, '做功', 3, 16)\n",
      "\n",
      "开始处理连续错误： 虽然这个世界每天都有人当妈妈，但真的将妈妈的角色做得好，不是每个人能成功做得到。「妈妈」不尽是个名子，也需要行动配著他。\n",
      "predict_text: 虽然这个世界每天都有人当妈妈，但真的将妈妈的角色做得好，不是每个人能成功做得到。「妈妈」不仅是个名字，也需要行动配合她。\n",
      "['著', '合', '注', '煮', '诸', '照', '作', '佐'] ['他', '她', '她', '他', '它', '地', '大', '人']\n",
      "None 0\n",
      "('虽然这个世界每天都有人当妈妈，但真的将妈妈的角色做得好，不是每个人能成功做得到。「妈妈」不仅是个名字，也需要行动配合她。', 0.9997900128364563, '合她', 2, 3)\n",
      "\n",
      "开始处理连续错误： 如果，没有她这么努力的一至帮我，今天我的身活就会改变了很多。\n",
      "predict_text: 如果，没有她这么努力的一直帮我，今天我的生活就会改变了很多。\n",
      "['的', '的', '的', '得', '德', '地', '个', '到'] ['一', '一', '以', '意', '医', '大', '因', '事']\n",
      "None 0\n",
      "('如果，没有她这么努力地一直帮我，今天我的生活就会改变了很多。', 0.9998115301132202, '地一', 2, 168)\n",
      "\n",
      "开始处理连续错误： 万一你不帮我们我们不会参加表，说不定我们可以帮学校布置地方或是发传单还有做广告。有的学生宁可大嫂也不要让大家笑他们所以我们真的需要你帮我们准备好了。\n",
      "predict_text: 万一你不帮我们我们不会参加表，说不定我们可以帮学校布置地方或是发传单还有做广告。有的学生宁可打扫也不要让大家笑他们所以我们真的需要你帮我们准备好了。\n",
      "['大', '打', '打', '搭', '答', '太', '带', '待'] ['嫂', '扫', '扫', '缫', '绍', '妈', '受', '媪']\n",
      "None 0\n",
      "('万一你不帮我们我们不会参加表，说不定我们可以帮学校布置地方或是发传单还有做广告。有的学生宁可打扫也不要让大家笑他们所以我们真的需要你帮我们准备好了。', 0.999894380569458, '打扫', 2, 16)\n",
      "\n",
      "开始处理连续错误： 对我来说，什么东西都可以买，不光安事件还是有空的，也不必只在在百花公词买的，在路边的摊位也可以买。\n",
      "predict_text: 对我来说，什么东西都可以买，不光安事间还是有空的，也不必只在在百货公司买的，在路边的摊位也可以买。\n",
      "['事', '事', '事', '时', '是', '什', '书', '系'] ['件', '间', '间', '见', '建', '情', '前', '迁']\n",
      "None 0\n",
      "\n",
      "开始处理连续错误： 波士顿是一个很漂亮的城，除了漂亮的房子波士顿还有有很多漂亮的河跟园和树。交通很方便，计程车，捷运，公车，甚么都有。\n",
      "predict_text: 波士顿是一个很漂亮的城，除了漂亮的房子波士顿还有有很多漂亮的河跟园和树。交通很方便，计程车，捷运，公车，甚么都有。\n",
      "['跟', '跟', '根', '更', '耕', '跼', '釭', '阬'] ['园', '园', '圆', '远', '猿', '湾', '弯', '周']\n",
      "None 0\n",
      "('波士顿是一个很漂亮的城，除了漂亮的房子波士顿还有有很多漂亮的河更远和树。交通很方便，计程车，捷运，公车，甚么都有。', 0.9781720638275146, '更远', 3, 18)\n",
      "\n",
      "开始处理连续错误： 我最希望是他们会爱上语言，就可以学很多语言。去外国可以认识很多的人，就可以借少。\n",
      "predict_text: 我最希望是他们会爱上语言，就可以学很多语言。去外国可以认识很多的人，就可以减少。\n",
      "['借', '减', '介', '解', '节', '惜', '决', '藉'] ['少', '少', '绍', '哨', '烧', '省', '小', '授']\n",
      "None 0\n",
      "('我最希望是他们会爱上语言，就可以学很多语言。去外国可以认识很多的人，就可以介绍。', 0.9998067021369934, '介绍', 2, 339)\n",
      "\n",
      "开始处理连续错误： 因为我觉得用学的语言跟同学说话不但让语言学生学到不一样的词很说法他们的进步也会很快，所以我也要给他们很多机会跟同学合作。\n",
      "predict_text: 因为我觉得用学的语言跟同学说话不但让语言学生学到不一样的词跟说法他们的进步也会很快，所以我也要给他们很多机会跟同学合作。\n",
      "['很', '跟', '恨', '哼', '狠', '跟', '和', '行'] ['说', '说', '说', '嗦', '所', '语', '书', '词']\n",
      "None 0\n",
      "('因为我觉得用学的语言跟同学说话不但让语言学生学到不一样的词和说法他们的进步也会很快，所以我也要给他们很多机会跟同学合作。', 0.9995936751365662, '和说', 2, 20)\n",
      "\n",
      "开始处理连续错误： 她的脸百百的、园园的，头发黑黑的，眼睛大大的。他穿的衣服也很适合她，看起来就算是一个女神不说，她的性格也算是非常温柔。\n",
      "predict_text: 她的脸白白的、圆圆的，头发黑黑的，眼睛大大的。她穿的衣服也很适合她，看起来就算是一个女神不说，她的性格也算是非常温柔。\n",
      "['园', '圆', '圆', '原', '远', '顽', '周', '玩'] ['园', '圆', '圆', '远', '元', '周', '宛', '里']\n",
      "None 0\n",
      "('她的脸白白的、远远的，头发黑黑的，眼睛大大的。她穿的衣服也很适合她，看起来就算是一个女神不说，她的性格也算是非常温柔。', 0.9999972581863403, '远远', 4, 71)\n",
      "\n",
      "开始处理连续错误： 中文对我说是非常很难，不论你会学一千多个字，你不会说很表尊都没有用。\n",
      "predict_text: 中文对我说是非常很难，不论你会学一千多个字，你不会说很表情都没有用。\n",
      "['表', '表', '表', '标', '鳔', '长', '写', '清'] ['尊', '情', '准', '遵', '谆', '从', '重', '状']\n",
      "None 0\n",
      "('中文对我说是非常很难，不论你会学一千多个字，你不会说很标准都没有用。', 0.9999836683273315, '标准', 3, 1320)\n",
      "\n",
      "开始处理连续错误： 成功大学的台湾学生一前已经希望了这一种表演。第一个，我要找谁要帮我准备那个地方跟找谁要找钱。最重要的事是找钱因为这个表演要很多钱，所以我希望朋友们可以帮我照前从公司、学校、或是从认识的人。\n",
      "predict_text: 成功大学的台湾学生以前已经希望了这一种表演。第一个，我要找谁要帮我准备那个地方跟找谁要赚钱。最重要的事是找钱因为这个表演要很多钱，所以我希望朋友们可以帮我提钱从公司、学校、或是从认识的人。\n",
      "['照', '提', '找', '招', '造', '超', '走', '朝'] ['前', '钱', '钱', '枪', '浅', '先', '荐', '例']\n",
      "None 0\n",
      "('成功大学的台湾学生以前已经希望了这一种表演。第一个，我要找谁要帮我准备那个地方跟找谁要赚钱。最重要的事是找钱因为这个表演要很多钱，所以我希望朋友们可以帮我找钱从公司、学校、或是从认识的人。', 0.9993155002593994, '找钱', 2, 68)\n",
      "\n",
      "开始处理连续错误： 过了一个礼拜又跟那群朋友约出来走走，很自信的我戴上了帽子出去，大家一看到就屏息了，他们都不但单不批评我反而夸奖了说，「好有型好漂亮等等」。在这个时刻我感到我买了这顶帽子是对的而从此只要戴上它不但很开心而且又有自信。\n",
      "predict_text: 过了一个礼拜又跟那群朋友约出来走走，很自信的我戴上了帽子出去，大家一看到就屏息了，他们都不但但不批评我反而夸张了说，「好有型好漂亮等等」。在这个时刻我感到我买了这顶帽子是对的而从此只要戴上它不但很开心而且又有自信。\n",
      "['但', '但', '单', '淡', '担', '坦', '大', '位'] ['单', '但', '但', '淡', '担', '坦', '严', '谈']\n",
      "None 0\n",
      "('过了一个礼拜又跟那群朋友约出来走走，很自信的我戴上了帽子出去，大家一看到就屏息了，他们都不但严不批评我反而夸张了说，「好有型好漂亮等等」。在这个时刻我感到我买了这顶帽子是对的而从此只要戴上它不但很开心而且又有自信。', 0.9994091987609863, '但严', 3, 7)\n",
      "\n",
      "开始处理连续错误： 您记者马？我去找您以后，说「请帮助我」，那天以后，您每天帮助我。\n",
      "predict_text: 您记得吗？我去找您以后，说「请帮助我」，那天以后，您每天帮助我。\n",
      "['者', '得', '着', '这', '折', '著', '知', '之'] ['马', '吗', '吗', '嘛', '妈', '么', '骗', '𫘪']\n",
      "None 0\n",
      "('您记得吗？我去找您以后，说「请帮助我」，那天以后，您每天帮助我。', 0.9999277591705322, '得吗', 2, 9)\n",
      "\n",
      "开始处理连续错误： 从有一天越来越厅的东您的课，我很感谢您的帮助。\n",
      "predict_text: 从有一天越来越听得教您的课，我很感谢您的帮助。\n",
      "['的', '得', '得', '的', '德', '地', '到', '了'] ['东', '教', '懂', '动', '冬', '本', '同', '通']\n",
      "None 0\n",
      "('从有一天越来越听得懂您的课，我很感谢您的帮助。', 0.9856444597244263, '得懂', 2, 20)\n",
      "\n",
      "开始处理连续错误： 现在我很兴奋，因为好久了我没跟你见面。我希望时间可以过真快让我们见面。\n",
      "predict_text: 现在我很兴奋，因为好久了我没跟你见面。我希望时间可以过真快让我们见面。\n",
      "['过', '过', '果', '国', '埚', '够', '逼', '达'] ['真', '真', '真', '整', '正', '长', '其', '值']\n",
      "None 0\n",
      "('现在我很兴奋，因为好久了我没跟你见面。我希望时间可以够长快让我们见面。', 0.9971697330474854, '够长', 0, 8)\n",
      "\n",
      "开始处理连续错误： 有可南的解释我觉得我增加一个知识。\n",
      "predict_text: 有柯南的解释我觉得我增加一个知识。\n",
      "['有', '有', '游', '友', '右', '左', '姚', '疢'] ['可', '柯', '柯', '克', '科', '肯', '歌', '格']\n",
      "None 0\n",
      "('有柯南的解释我觉得我增加一个知识。', 0.9381193518638611, '有柯', 3, 4)\n",
      "\n",
      "开始处理连续错误： 公寓在师大夜市旁边，但是情况很案情。交通非常方便，不管要去哪里，捷运、公车都有。\n",
      "predict_text: 公寓在师大夜市旁边，但是情况很热静。交通非常方便，不管要去哪里，捷运、公车都有。\n",
      "['案', '热', '安', '按', '暗', '桑', '森', '痳'] ['情', '静', '轻', '清', '亲', '静', '特', '净']\n",
      "None 0\n",
      "('公寓在师大夜市旁边，但是情况很安静。交通非常方便，不管要去哪里，捷运、公车都有。', 0.9999983310699463, '安静', 3, 78)\n",
      "\n",
      "开始处理连续错误： 大楼附近你会有很方便的交通站，我觉得及脚踏车比较好，因为你会有停脚踏车的地方在外面啊、通过后面的街啊、家里啊，搜仪我觉得没有问题。\n",
      "predict_text: 大楼附近你会有很方便的交通站，我觉得骑脚踏车比较好，因为你会有停脚踏车的地方在外面啊、通过后面的街啊、家里啊，搜仪我觉得没有问题。\n",
      "['搜', '搜', '搜', '首', '受', '所', '理', '几'] ['仪', '仪', '以', '一', '易', '样', '对', '尾']\n",
      "None 0\n",
      "('大楼附近你会有很方便的交通站，我觉得骑脚踏车比较好，因为你会有停脚踏车的地方在外面啊、通过后面的街啊、家里啊，所以我觉得没有问题。', 0.9999997615814209, '所以', 2, 2449)\n",
      "\n",
      "detect_precision=0.768080, detect_recall=0.779747, detect_Fscore=0.773869\n",
      "correct_precision=0.975649, correct_recall=0.760759, correct_Fscore=0.854908\n",
      "dc_joint_precision=1.000000, dc_joint_recall=1.000000, dc_joint_Fscore=1.000000\n",
      "detect_sent_precision=0.703985, detect_sent_recall=0.698682, detect_Fscore=0.701323\n",
      "correct_sent_precision=0.686907, correct_sent_recall=0.681733, correct_Fscore=0.684310\n"
     ]
    }
   ],
   "source": [
    "#sighan14\n",
    "tokenizer = _get_tokenizer()\n",
    "predict_labels = []\n",
    "input_ids = []\n",
    "sentence_ids = []\n",
    "sig = \"14\"\n",
    "test_data_arg = args.dataset.test\n",
    "test_data_arg.text_path = \"./resources/sighan{}/TestInput.txt\".format(sig)\n",
    "test_data_arg.label_path = \"./resources/sighan{}/TestTruth.txt\".format(sig)\n",
    "\n",
    "test_dataset = get_dataset(test_data_arg)\n",
    "from tqdm  import tqdm_notebook\n",
    "with torch.no_grad():\n",
    "    for i in tqdm_notebook(range(len(test_dataset))):\n",
    "        raw_item = test_dataset.__getitem__(i)\n",
    "        original_text = raw_item[1]\n",
    "        inputs = sighan_error_detec_csc([raw_item])\n",
    "        inputs = step(inputs)\n",
    "        detec_logits = detec_model(inputs)\n",
    "        detec_predicts = detec_logits.argmax(dim=2).cpu().numpy().tolist()\n",
    "        detec_tokens = tokenizer.convert_ids_to_tokens(detec_predicts[0])[1:-1]\n",
    "        \n",
    "        \n",
    "        #对连续错误进行处理\n",
    "        predict_text = \"\".join(detec_tokens)\n",
    "        logits = detec_logits.softmax(dim=2)\n",
    "        tensor_ids = possible_err_positions(logits)\n",
    "\n",
    "\n",
    "        new_error_outer = False\n",
    "        for i in range(len(original_text) -1):\n",
    "            if original_text[i] != predict_text[i] and original_text[i+1] != predict_text[i+1] and (predict_text[i] not in get_candidates(original_text[i])[0]  or predict_text[i+1] not in get_candidates(original_text[i+1])[0]):\n",
    "                import re\n",
    "                if len(re.findall(r'([\\u2E80-\\u9FFF]+)',original_text[i] )) == 0:\n",
    "                    continue\n",
    "                new_error_outer = True\n",
    "                tensor_ids = torch.LongTensor([i+1,i+2]).to(tensor_ids.device)\n",
    "                break\n",
    "                \n",
    "        target = 0.9\n",
    "\n",
    "        if len(tensor_ids) == 2 and tensor_ids[0]+ 1 == tensor_ids[1] or new_error_outer:\n",
    "            print(\"开始处理连续错误：\", raw_item[1])\n",
    "            modified_predict_ids = solve_bi_err(tensor_ids,logits,raw_item[1],target)\n",
    "            if modified_predict_ids is not None:\n",
    "                detec_predicts = modified_predict_ids\n",
    "            print()\n",
    "        \n",
    "            \n",
    "        tokenizer = _get_tokenizer()\n",
    "        \n",
    "        input_ids.extend(inputs['input_ids'].detach().cpu().numpy().tolist())\n",
    "  \n",
    "\n",
    "        predict_labels.extend(detec_predicts)\n",
    "        \n",
    "        sentence_ids.extend(inputs['sentence_ids'])\n",
    "        \n",
    "        predict_text = \"\".join(tokenizer.convert_ids_to_tokens(detec_predicts[0][1:-1]))\n",
    "\n",
    "\n",
    "save_path = \"../test_file\"\n",
    "write_csc_predictions(input_ids, predict_labels, sentence_ids, save_path)\n",
    "metrics = compute_csc_metrics_by_file(save_path,\"./resources/sighan{}/TestTruth.txt\".format(sig), show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3bee192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\论文代码和数据备份\\CSC\n",
      "998 samples found in ./resources/sighan13/TestInput.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-a522bc362d05>:14: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(len(test_dataset))):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75af7b8c74424595bbb20edf267122dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/998 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理连续错误： 爱迪生，小时候被所有人骂他是低能儿，可是他永有不屈不侥像上的经神，从火海中在站起来，后来他成为了不起的发明家。\n",
      "predict_text: 爱迪生，小时候被所有人骂他是低能儿，可是他拥有不屈不屈向上的精神，从火海中再站起来，后来他成为了不起的发明家。\n",
      "['侥', '屈', '骄', '交', '娇', '肖', '朽', '挠'] ['像', '向', '向', '像', '想', '心', '强', '信']\n",
      "None 0\n",
      "('爱迪生，小时候被所有人骂他是低能儿，可是他拥有不屈不肖像上的精神，从火海中再站起来，后来他成为了不起的发明家。', 0.9984805583953857, '肖像', 2, 47)\n",
      "\n",
      "开始处理连续错误： 身为学生，教室是最好的游乐场，因为一天可以利用的时间一半都在教室，从桌椅到门窗，从黑板到垃圾桶，都可以把完、欣赏。\n",
      "predict_text: 身为学生，教室是最好的游乐场，因为一天可以利用的时间一半都在教室，从桌椅到门窗，从黑板到垃圾桶，都可以玩玩、欣赏。\n",
      "['把', '玩', '吧', '八', '巴', '办', '抓', '犯'] ['完', '玩', '玩', '碗', '弯', '园', '亮', '圆']\n",
      "None 0\n",
      "('身为学生，教室是最好的游乐场，因为一天可以利用的时间一半都在教室，从桌椅到门窗，从黑板到垃圾桶，都可以玩玩、欣赏。', 0.9998973608016968, '玩玩', 2, 19)\n",
      "\n",
      "开始处理连续错误： 我喜欢秋天的风，也喜欢秋天的雨，不管是中秋月圆，还是牛郎与织女的微美故事。都深深的烧印在我的脑海中。秋天虽然有时令人感到凄凉，但仔细一看，倒发现其中也孕育著许多生命。秋天的风呼呼吹来，它将带著我的思念传到那充满欢笑声的小学。\n",
      "predict_text: 我喜欢秋天的风，也喜欢秋天的雨，不管是中秋月圆，还是牛郎与织女的唯美故事。都深深地烙印在我的脑海中。秋天虽然有时令人感到凄凉，但仔细一看，倒发现其中也孕育著许多生命。秋天的风呼呼吹来，它将带著我的思念传到那充满欢笑声的小学。\n",
      "['的', '地', '的', '得', '德', '地', '个', '到'] ['烧', '烙', '缫', '韶', '邵', '烫', '嗾', '铙']\n",
      "None 0\n",
      "('我喜欢秋天的风，也喜欢秋天的雨，不管是中秋月圆，还是牛郎与织女的唯美故事。都深深地烙印在我的脑海中。秋天虽然有时令人感到凄凉，但仔细一看，倒发现其中也孕育著许多生命。秋天的风呼呼吹来，它将带著我的思念传到那充满欢笑声的小学。', 0.9999855756759644, '地烙', 1, 0)\n",
      "\n",
      "开始处理连续错误： 有一次，我和同学有说有笑的走进教室。我不经意的毒了黑板一眼，竟看到黑板不知何时被人贴上了一面「手绘国旗」！看到这幅景像，我不禁笑了起来，我们班真是太有创意了！那面手绘国旗到现在还贴在教室里呢！\n",
      "predict_text: 有一次，我和同学有说有笑地走进教室。我不经意地毒了黑板一眼，竟看到黑板不知何时被人贴上了一面「手绘国旗」！看到这幅景象，我不禁笑了起来，我们班真是太有创意了！那面手绘国旗到现在还贴在教室里呢！\n",
      "['的', '地', '的', '得', '德', '地', '到', '了'] ['毒', '毒', '赌', '椟', '读', '猜', '酴', '湎']\n",
      "None 0\n",
      "('有一次，我和同学有说有笑地走进教室。我不经意地读了黑板一眼，竟看到黑板不知何时被人贴上了一面「手绘国旗」！看到这幅景象，我不禁笑了起来，我们班真是太有创意了！那面手绘国旗到现在还贴在教室里呢！', 0.9179264903068542, '地读', 2, 51)\n",
      "\n",
      "开始处理连续错误： 有一位学问渊博的学者向一位和尚问襌，那和尚先请学者喝茶，在倒茶时，和尚却一直倒一直到茶都满出来了还不停止，并笑著对学者说：「你就像这只茶杯，已经装满了自己的想法，怎么听我说禅呢？」\n",
      "predict_text: 有一位学问渊博的学者向一位和尚问dan，那和尚先请学者喝茶，在倒茶时，和尚却一直倒一直到茶都满出来了还不停止，并笑著对学者说：「你就像这只茶杯，已经装满了自己的想法，怎么听我说禅呢？」\n",
      "['襌', 'd'] ['，', 'a']\n",
      "None 0\n",
      "('有一位学问渊博的学者向一位和尚问dan，那和尚先请学者喝茶，在倒茶时，和尚却一直倒一直到茶都满出来了还不停止，并笑著对学者说：「你就像这只茶杯，已经装满了自己的想法，怎么听我说禅呢？」', 0.996126115322113, 'da', 0, 0)\n",
      "\n",
      "开始处理连续错误： 在行船的路上，可能会迷失方向，可能会感觉傍惶，但千万要记得：「人不可以改变生命的长度，但生命的宽度却等著我们去拓展。」\n",
      "predict_text: 在行船的路上，可能会迷失方向，可能会感觉傍徨，但千万要记得：「人不可以改变生命的长度，但生命的宽度却等著我们去拓展。」\n",
      "['傍', '傍', '帮', '棒', '磅', '徬', '旁', '仿'] ['惶', '徨', '徨', '慌', '晃', '慢', '愕', '迫']\n",
      "None 0\n",
      "\n",
      "开始处理连续错误： 为自己订的人生方向就是应该去完成，不应该整天只想要玩，而荒废了课业。如果在这样继续玩秽愒日，浪废光阴的话，也许我可能会后悔一辈子。\n",
      "predict_text: 为自己订的人生方向就是应该去完成，不应该整天只想要玩，而荒废了课业。如果在这样继续玩垃[UNK]日，浪费光阴的话，也许我可能会后悔一辈子。\n",
      "['秽', '垃', '虺', '挥', '会', '岁', '穑', '秾'] ['愒', '[', '凯', '开', '恺', '疠', '𪟝', '垓']\n",
      "None 0\n",
      "\n",
      "开始处理连续错误： 人的一生总是波涛凶涌，有时甜，有时苦，有时乐，则有时悲，若想好好把握这辈子，就新颓该前往哪个「方向」。\n",
      "predict_text: 人的一生总是波涛汹涌，有时甜，有时苦，有时乐，则有时悲，若想好好把握这辈子，就知知该前往那个「方向」。\n",
      "['新', '知', '心', '形', '信', '想', '更', '相'] ['颓', '知', '推', '腿', '退', '须', '顾', '对']\n",
      "None 0\n",
      "('人的一生总是波涛汹涌，有时甜，有时苦，有时乐，则有时悲，若想好好把握这辈子，就心知该前往那个「方向」。', 0.9896023273468018, '心知', 1, 10)\n",
      "\n",
      "开始处理连续错误： 我从前有打算往艺术这个方向走，但并不是很确定。我现在就是在为未来而铺路，是平坦的，是崎区的，都要看我现在怎样去打造自己的星光大道。\n",
      "predict_text: 我从前有打算往艺术这个方向走，但并不是很确定。我现在就是在为未来而铺路，是平坦的，是崎区的，都要看我现在怎样去打造自己的星光大道。\n",
      "['崎', '崎', '奇', '歧', '琪', '基', '棘', '忌'] ['区', '区', '岖', '曲', '去', '凹', '居', '板']\n",
      "None 0\n",
      "('我从前有打算往艺术这个方向走，但并不是很确定。我现在就是在为未来而铺路，是平坦的，是崎岖的，都要看我现在怎样去打造自己的星光大道。', 0.999984860420227, '崎岖', 3, 38)\n",
      "\n",
      "开始处理连续错误： 假如无法达成的话，我也可以中途改行，去帮家里的水果店货送货，这样没事情的时候我也可以专注在自己的电脑游戏，顺变了解此游戏是如何创造，为何会如此吸引小朋友玩此游戏呢？\n",
      "predict_text: 假如无法达成的话，我也可以中途改行，去帮家里的水果店或送货，这样没事情的时候我也可以专注在自己的电脑游戏，顺便了解此游戏是如何创造，为何会如此吸引小朋友玩此游戏呢？\n",
      "['店', '店', '点', '电', '殿', '甜', '图', '底'] ['货', '或', '或', '获', '祸', '员', '买', '和']\n",
      "None 0\n",
      "('假如无法达成的话，我也可以中途改行，去帮家里的水果店员送货，这样没事情的时候我也可以专注在自己的电脑游戏，顺便了解此游戏是如何创造，为何会如此吸引小朋友玩此游戏呢？', 0.9999861717224121, '店员', 2, 27)\n",
      "\n",
      "开始处理连续错误： 有些人把快乐建筑在别人的身上，这种行为不是快乐，而是遭踏！不应该效法他们。有啊！不只和好朋友聊天，我们时常约出来一起打排球呢！这项运动，不旦可以帮助我们增进感情，还可以使我们都很开心。\n",
      "predict_text: 有些人把快乐建筑在别人的身上，这种行为不是快乐，而是糟踏！不应该效法他们。有啊！不只和好朋友聊天，我们时常约出来一起打排球呢！这项运动，不但可以帮助我们增进感情，还可以使我们都很开心。\n",
      "['遭', '糟', '糟', '照', '招', '超', '违', '揍'] ['踏', '踏', '蹋', '塌', '他', '踩', '趴', '蹈']\n",
      "None 0\n",
      "\n",
      "detect_precision=0.859690, detect_recall=0.878068, detect_Fscore=0.868782\n",
      "correct_precision=0.994590, correct_recall=0.873317, correct_Fscore=0.930017\n",
      "dc_joint_precision=1.000000, dc_joint_recall=1.000000, dc_joint_Fscore=1.000000\n",
      "detect_sent_precision=0.867033, detect_sent_recall=0.790581, detect_Fscore=0.827044\n",
      "correct_sent_precision=0.862637, correct_sent_recall=0.786573, correct_Fscore=0.822851\n"
     ]
    }
   ],
   "source": [
    "#sighan13\n",
    "tokenizer = _get_tokenizer()\n",
    "predict_labels = []\n",
    "input_ids = []\n",
    "sentence_ids = []\n",
    "sig = \"13\"\n",
    "test_data_arg = args.dataset.test\n",
    "test_data_arg.text_path = \"./resources/sighan{}/TestInput.txt\".format(sig)\n",
    "test_data_arg.label_path = \"./resources/sighan{}/TestTruth.txt\".format(sig)\n",
    "\n",
    "test_dataset = get_dataset(test_data_arg)\n",
    "from tqdm  import tqdm_notebook\n",
    "with torch.no_grad():\n",
    "    for i in tqdm_notebook(range(len(test_dataset))):\n",
    "        raw_item = test_dataset.__getitem__(i)\n",
    "        original_text = raw_item[1]\n",
    "        inputs = sighan_error_detec_csc([raw_item])\n",
    "        inputs = step(inputs)\n",
    "        detec_logits = detec_model(inputs)\n",
    "        detec_predicts = detec_logits.argmax(dim=2).cpu().numpy().tolist()\n",
    "        detec_tokens = tokenizer.convert_ids_to_tokens(detec_predicts[0])[1:-1]\n",
    "        \n",
    "        \n",
    "        #对连续错误进行处理\n",
    "        predict_text = \"\".join(detec_tokens)\n",
    "        logits = detec_logits.softmax(dim=2)\n",
    "        tensor_ids = possible_err_positions(logits)\n",
    "\n",
    "\n",
    "        new_error_outer = False\n",
    "        for i in range(len(original_text) -1):\n",
    "            if original_text[i] != predict_text[i] and original_text[i+1] != predict_text[i+1] and (predict_text[i] not in get_candidates(original_text[i])[0]  or predict_text[i+1] not in get_candidates(original_text[i+1])[0]):\n",
    "                import re\n",
    "                if len(re.findall(r'([\\u2E80-\\u9FFF]+)',original_text[i] )) == 0:\n",
    "                    continue\n",
    "                new_error_outer = True\n",
    "                tensor_ids = torch.LongTensor([i+1,i+2]).to(tensor_ids.device)\n",
    "                break\n",
    "                \n",
    "        target = 0.9\n",
    "\n",
    "        if len(tensor_ids) == 2 and tensor_ids[0]+ 1 == tensor_ids[1] or new_error_outer:\n",
    "            print(\"开始处理连续错误：\", raw_item[1])\n",
    "            modified_predict_ids = solve_bi_err(tensor_ids,logits,raw_item[1],target)\n",
    "            if modified_predict_ids is not None:\n",
    "                detec_predicts = modified_predict_ids\n",
    "            print()\n",
    "        \n",
    "            \n",
    "        tokenizer = _get_tokenizer()\n",
    "        \n",
    "        input_ids.extend(inputs['input_ids'].detach().cpu().numpy().tolist())\n",
    "  \n",
    "\n",
    "        predict_labels.extend(detec_predicts)\n",
    "        \n",
    "        sentence_ids.extend(inputs['sentence_ids'])\n",
    "        \n",
    "        predict_text = \"\".join(tokenizer.convert_ids_to_tokens(detec_predicts[0][1:-1]))\n",
    "\n",
    "\n",
    "save_path = \"../test_file\"\n",
    "write_csc_predictions(input_ids, predict_labels, sentence_ids, save_path,is_13=True)\n",
    "metrics = compute_csc_metrics_by_file(save_path,\"./resources/sighan{}/TestTruth.txt\".format(sig), show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d834999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e538b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f217044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904429ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
